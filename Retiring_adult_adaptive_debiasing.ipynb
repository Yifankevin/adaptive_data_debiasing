{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e334b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_LIST = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI',\n",
    "              'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI',\n",
    "              'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC',\n",
    "              'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT',\n",
    "              'VT', 'VA', 'WA', 'WV', 'WI', 'WY', 'PR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f736386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "from folktables import ACSDataSource, ACSIncome\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import beta\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from responsibly.fairness.interventions.threshold import find_thresholds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = pd.DataFrame()\n",
    "\n",
    "for state in STATE_LIST:\n",
    "    acs_data = pd.concat([acs_data, data_source.get_data(states=[state], download=False)])\n",
    "\n",
    "features, label, group = ACSIncome.df_to_numpy(acs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_constarint = ['single', 'min_cost', 'independence', 'fnr']\n",
    "fair = 3\n",
    "\n",
    "# Define true parameter settings\n",
    "min_1_group1 = 0\n",
    "max_1_group1 = 1\n",
    "min_0_group1 = 0\n",
    "max_0_group1 = 1\n",
    "min_1_group0 = 0\n",
    "max_1_group0 = 1\n",
    "min_0_group0 = 0\n",
    "max_0_group0 = 1\n",
    "\n",
    "reference_quantile_0 = 0.6\n",
    "reference_quantile_1 = 0.5\n",
    "exploration_porb_group1 = 1\n",
    "exploration_porb_group0 = 1\n",
    "batchsize_init = 6000\n",
    "batchsize_additional = 2000\n",
    "\n",
    "# Defining other parameters\n",
    "TP_group1 = 0\n",
    "FP_group1 = 0\n",
    "FN_group1 = 0\n",
    "TN_group1 = 0\n",
    "TP_group0 = 0\n",
    "FP_group0 = 0\n",
    "FN_group0 = 0\n",
    "TN_group0 = 0\n",
    "\n",
    "TP_oracle_group1 = 0 \n",
    "FP_oracle_group1 = 0\n",
    "FN_oracle_group1 = 0\n",
    "TN_oracle_group1 = 0\n",
    "TP_oracle_group0 = 0 \n",
    "FP_oracle_group0 = 0\n",
    "FN_oracle_group0 = 0\n",
    "TN_oracle_group0 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6d999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, label, train_size = 0.99999, shuffle=True, random_state = 0)\n",
    "\n",
    "classifier = LogisticRegression().fit(X_train, y_train)\n",
    "prob = classifier.predict_proba(features)[:,1]\n",
    "\n",
    "original = pd.DataFrame(features)\n",
    "original[\"Prob\"] = prob\n",
    "original['Target'] = label\n",
    "original['Race'] = group\n",
    "original\n",
    "\n",
    "# Split four groups data for true parameters mapping\n",
    "grouphigh_label1 = original[(original[\"Target\"]==True) & (original[\"Race\"]==1)][\"Prob\"]\n",
    "grouphigh_label0 = original[(original[\"Target\"]==False) & (original[\"Race\"]==1)][\"Prob\"]\n",
    "grouplow_label1 = original[(original[\"Target\"]==True) & (original[\"Race\"]!=1)][\"Prob\"]\n",
    "grouplow_label0 = original[(original[\"Target\"]==False) & (original[\"Race\"]!=1)][\"Prob\"]\n",
    "\n",
    "# Reindex four groups data\n",
    "grouphigh_label1.reset_index(inplace=True, drop=True)\n",
    "grouphigh_label0.reset_index(inplace=True, drop=True)\n",
    "grouplow_label1.reset_index(inplace=True, drop=True)\n",
    "grouplow_label0.reset_index(inplace=True, drop=True)\n",
    "\n",
    "a,b1_true_group1,c,d = beta.fit(grouphigh_label1,floc=min_1_group1,fscale=max_1_group1-min_1_group1)\n",
    "a,b0_true_group1,c,d = beta.fit(grouphigh_label0,floc=min_0_group1,fscale=max_0_group1-min_0_group1)\n",
    "a,b1_true_group0,c,d = beta.fit(grouplow_label1,floc=min_1_group0,fscale=max_1_group0-min_1_group0)\n",
    "a,b0_true_group0,c,d = beta.fit(grouplow_label0,floc=min_0_group0,fscale=max_0_group0-min_0_group0)\n",
    "\n",
    "x = np.arange(0.01,10,0.01)\n",
    "def f(alpha):\n",
    "    return abs(beta.cdf(np.quantile(grouphigh_label1,reference_quantile_1), alpha, b1_true_group1, loc=min_1_group1, scale=max_1_group1-min_1_group1) - reference_quantile_1)\n",
    "a1_true_group1 = np.argmin(f(x))*0.01\n",
    "\n",
    "def f(alpha):\n",
    "    return abs(beta.cdf(np.quantile(grouphigh_label0,reference_quantile_0), alpha, b0_true_group1, loc=min_0_group1, scale=max_0_group1-min_0_group1) - reference_quantile_0)\n",
    "a0_true_group1 = np.argmin(f(x))*0.01\n",
    "\n",
    "def f(alpha):\n",
    "    return abs(beta.cdf(np.quantile(grouplow_label1,reference_quantile_1), alpha, b1_true_group0, loc=min_1_group0, scale=max_1_group0-min_1_group0) - reference_quantile_1)\n",
    "a1_true_group0 = np.argmin(f(x))*0.01\n",
    "\n",
    "def f(alpha):\n",
    "    return abs(beta.cdf(np.quantile(grouplow_label0,reference_quantile_0), alpha, b0_true_group0, loc=min_0_group0, scale=max_0_group0-min_0_group0) - reference_quantile_0)\n",
    "a0_true_group0 = np.argmin(f(x))*0.01\n",
    "\n",
    "print (a1_true_group1,b1_true_group1,a0_true_group1,b0_true_group1,\n",
    "       a1_true_group0,b1_true_group0,a0_true_group0,b0_true_group0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fc0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first (Initial_fit_portion) rows to find initial assumed distribution\n",
    "Initial_fit_portion = 0.03\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, train_size = Initial_fit_portion, shuffle=True, random_state = 0)\n",
    "\n",
    "classifier = LogisticRegression().fit(X_train, y_train)\n",
    "prob = classifier.predict_proba(features)[:,1]\n",
    "original[\"Prob\"] = prob\n",
    "First_n_data = original.head(round(Initial_fit_portion*len(features)))\n",
    "Initialhigh_label1 = First_n_data[(First_n_data[\"Target\"]==True) & (First_n_data[\"Race\"]==1)][\"Prob\"]\n",
    "Initialhigh_label0 = First_n_data[(First_n_data[\"Target\"]==False) & (First_n_data[\"Race\"]==1)][\"Prob\"]\n",
    "Initiallow_label1 = First_n_data[(First_n_data[\"Target\"]==True) & (First_n_data[\"Race\"]!=1)][\"Prob\"]\n",
    "Initiallow_label0 = First_n_data[(First_n_data[\"Target\"]==False) & (First_n_data[\"Race\"]!=1)][\"Prob\"]\n",
    "\n",
    "# Reindex\n",
    "Initialhigh_label1.reset_index(inplace=True, drop=True)\n",
    "Initialhigh_label0.reset_index(inplace=True, drop=True)\n",
    "Initiallow_label1.reset_index(inplace=True, drop=True)\n",
    "Initiallow_label0.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Fix beta parameter\n",
    "b1_init_group1 = b1_true_group1\n",
    "b0_init_group1 = b0_true_group1\n",
    "b1_init_group0 = b1_true_group0\n",
    "b0_init_group0 = b0_true_group0\n",
    "\n",
    "b1_group1 = b1_init_group1\n",
    "b0_group1 = b0_init_group1\n",
    "b1_group0 = b1_init_group0\n",
    "b0_group0 = b0_init_group0\n",
    "\n",
    "# Find initial alpha parameter by fixing beta value\n",
    "def f(alpha):\n",
    "    return abs(beta.cdf(np.quantile(Initialhigh_label1,reference_quantile_1), alpha, b1_true_group1, loc=min_1_group1, scale=max_1_group1-min_1_group1) - reference_quantile_1)\n",
    "a1_init_group1 = np.argmin(f(x))*0.01\n",
    "\n",
    "def f(alpha):\n",
    "    return abs(beta.cdf(np.quantile(Initialhigh_label0,reference_quantile_0), alpha, b0_true_group1, loc=min_0_group1, scale=max_0_group1-min_0_group1) - reference_quantile_0)\n",
    "a0_init_group1 = np.argmin(f(x))*0.01\n",
    "\n",
    "def f(alpha):\n",
    "    return abs(beta.cdf(np.quantile(Initiallow_label1,reference_quantile_1), alpha, b1_true_group0, loc=min_1_group0, scale=max_1_group0-min_1_group0) - reference_quantile_1)\n",
    "a1_init_group0 = np.argmin(f(x))*0.01\n",
    "\n",
    "def f(alpha):\n",
    "    return abs(beta.cdf(np.quantile(Initiallow_label0,reference_quantile_0), alpha, b0_true_group0, loc=min_0_group0, scale=max_0_group0-min_0_group0) - reference_quantile_0)\n",
    "a0_init_group0 = np.argmin(f(x))*0.01\n",
    "\n",
    "a1_group1 = a1_init_group1\n",
    "a0_group1 = a0_init_group1\n",
    "a1_group0 = a1_init_group0\n",
    "a0_group0 = a0_init_group0\n",
    "print (a1_group1,b1_group1,a0_group1,b0_group1,\n",
    "       a1_group0,b1_group0,a0_group0,b0_group0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b70a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65265a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the confusion matrix given threshold\n",
    "def CM(Y_test,y_pred,threshold):\n",
    "    # The Confusion Matrix given a threshold\n",
    "    y_pred = np.where(y_pred>threshold,1,0)\n",
    "    cm = pd.DataFrame(confusion_matrix(Y_test,y_pred))\n",
    "    cm.rename(columns={0:'Pred_neg', 1:'Pred_pos'},\n",
    "         index = {0:'Actual_neg',1:'Actual_pos'},inplace=True)\n",
    "    cm['Total'] = cm['Pred_neg'] + cm['Pred_pos'] \n",
    "    rowsum = cm.sum()\n",
    "    rowsum.name = 'Total'\n",
    "    cm = cm.append(rowsum.transpose())\n",
    "    \n",
    "    # TP/TN/FP/FN/TPR/FPR\n",
    "    P = cm['Total']['Actual_pos']\n",
    "    N = cm['Total']['Actual_neg']\n",
    "    TP = cm['Pred_pos']['Actual_pos']\n",
    "    TN = cm['Pred_neg']['Actual_neg']\n",
    "    FP = cm['Pred_pos']['Actual_neg']\n",
    "    FN = cm['Pred_neg']['Actual_pos']\n",
    "    TPR = np.round(TP/P,2)\n",
    "    FPR = np.round(FP/N,2)\n",
    "    return(cm, TPR, FPR)\n",
    "\n",
    "# Define the cost matrix\n",
    "COST_MATRIX = [[0, -3/6],\n",
    "               [0,  3/6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ae9bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find fair classifier for the inital training set\n",
    "\n",
    "# Find the parameters proportions, base_rate and base_rates \n",
    "proportions = {'White': (len(Initialhigh_label1) + len(Initialhigh_label0))/(len(Initialhigh_label1) + len(Initialhigh_label0) + len(Initiallow_label1) + len(Initiallow_label0)), \n",
    "               'Non-White': 1 - (len(Initialhigh_label1) + len(Initialhigh_label0))/(len(Initialhigh_label1) + len(Initialhigh_label0) + len(Initiallow_label1) + len(Initiallow_label0))}\n",
    "base_rate = (len(Initialhigh_label1) + len(Initiallow_label1))/(len(Initialhigh_label1) + len(Initialhigh_label0) + len(Initiallow_label1) + len(Initiallow_label0))\n",
    "d = {'White': len(Initialhigh_label1)/(len(Initialhigh_label1) + len(Initialhigh_label0)), 'Non-White': len(Initiallow_label1)/(len(Initiallow_label1) + len(Initiallow_label0))}\n",
    "base_rates = pd.Series(data = d, index = ['White','Non-White'])\n",
    "\n",
    "# Find the ROC curve by construting confusion matrix \n",
    "yprob1 = First_n_data[First_n_data[\"Race\"]==1][\"Prob\"]\n",
    "yprob0 = First_n_data[First_n_data[\"Race\"]!=1][\"Prob\"]\n",
    "  \n",
    "TPR1 = []\n",
    "FPR1 = []\n",
    "threshold1 = []\n",
    "for threshold in range(201,0,-1):\n",
    "    cm, TPR, FPR = CM(First_n_data[First_n_data[\"Race\"]==1][\"Target\"],yprob1,threshold/200)\n",
    "    TPR1 = TPR1 + [TPR]\n",
    "    FPR1 = FPR1 + [FPR]\n",
    "    threshold1 = threshold1 + [threshold/200]\n",
    "TPR1 = np.array(TPR1)\n",
    "FPR1 = np.array(FPR1)\n",
    "threshold1 = np.array(threshold1)\n",
    "\n",
    "TPR0 = []\n",
    "FPR0 = []\n",
    "threshold0 = []\n",
    "for threshold in range(201,0,-1):\n",
    "    cm, TPR, FPR = CM(First_n_data[First_n_data[\"Race\"]!=1][\"Target\"],yprob0,threshold/200)\n",
    "    TPR0 = TPR0 + [TPR]\n",
    "    FPR0 = FPR0 + [FPR]\n",
    "    threshold0 = threshold0 + [threshold/200]\n",
    "TPR0 = np.array(TPR0)\n",
    "FPR0 = np.array(FPR0)\n",
    "threshold0 = np.array(threshold0)\n",
    "\n",
    "# Find the parameter rocs \n",
    "rocs = {'White': (FPR1, TPR1, threshold1), 'Non-White': (FPR0, TPR0,threshold0)}\n",
    "\n",
    "# Find the initial fairness classifier_1 and classifier_2 \n",
    "thresholds_data = find_thresholds(rocs, proportions, base_rate, base_rates, COST_MATRIX)\n",
    "if fair != 0:\n",
    "    classifier_group1 = thresholds_data[fair_constarint[fair]][0]['White']\n",
    "    classifier_group0 = thresholds_data[fair_constarint[fair]][0]['Non-White']\n",
    "else: \n",
    "    classifier_group1 = thresholds_data[fair_constarint[fair]][0]\n",
    "    classifier_group0 = thresholds_data[fair_constarint[fair]][0]\n",
    "#print (classifier_group1, classifier_group0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c7815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find oracle fair classifier for the entire set\n",
    "\n",
    "# Find the parameters proportions, base_rate and base_rates \n",
    "proportions = {'White': (len(grouphigh_label1) + len(grouphigh_label0))/(len(grouphigh_label1) + len(grouphigh_label0) + len(grouplow_label1) + len(grouplow_label1)), \n",
    "               'Non-White': 1 - (len(grouphigh_label1) + len(grouphigh_label0))/(len(grouphigh_label1) + len(grouphigh_label0) + len(grouplow_label1) + len(grouplow_label1))}\n",
    "base_rate = (len(grouphigh_label1) + len(grouplow_label1))/(len(grouphigh_label1) + len(grouphigh_label0) + len(grouplow_label1) + len(grouplow_label1))\n",
    "d = {'White': len(grouphigh_label1)/(len(grouphigh_label1) + len(grouphigh_label0)), 'Non-White': len(grouplow_label1)/(len(grouplow_label1) + len(grouplow_label0))}\n",
    "base_rates = pd.Series(data = d, index = ['White','Non-White'])\n",
    "\n",
    "# Find the ROC curve by construting confusion matrix \n",
    "yprob1 = original[original[\"Race\"]==1][\"Prob\"]\n",
    "yprob0 = original[original[\"Race\"]!=1][\"Prob\"]\n",
    "\n",
    "TPR1 = []\n",
    "FPR1 = []\n",
    "threshold1 = []\n",
    "for threshold in range(201,0,-1):\n",
    "    cm, TPR, FPR = CM(original[original[\"Race\"]==1][\"Target\"],yprob1,threshold/200)\n",
    "    TPR1 = TPR1 + [TPR]\n",
    "    FPR1 = FPR1 + [FPR]\n",
    "    threshold1 = threshold1 + [threshold/200]\n",
    "TPR1 = np.array(TPR1)\n",
    "FPR1 = np.array(FPR1)\n",
    "threshold1 = np.array(threshold1)\n",
    "\n",
    "TPR0 = []\n",
    "FPR0 = []\n",
    "threshold0 = []\n",
    "for threshold in range(201,0,-1):\n",
    "    cm, TPR, FPR = CM(original[original[\"Race\"]!=1][\"Target\"],yprob0,threshold/200)\n",
    "    TPR0 = TPR0 + [TPR]\n",
    "    FPR0 = FPR0 + [FPR]\n",
    "    threshold0 = threshold0 + [threshold/200]\n",
    "TPR0 = np.array(TPR0)\n",
    "FPR0 = np.array(FPR0)\n",
    "threshold0 = np.array(threshold0)\n",
    "\n",
    "# Find the parameter rocs \n",
    "rocs = {'White': (FPR1, TPR1, threshold1), 'Non-White': (FPR0, TPR0,threshold0)}\n",
    "\n",
    "# Find the initial fairness classifier_1 and classifier_2 \n",
    "thresholds_data = find_thresholds(rocs, proportions, base_rate, base_rates, COST_MATRIX)\n",
    "if fair != 0:\n",
    "    classifier_oracle_value_1 = thresholds_data[fair_constarint[fair]][0]['White']\n",
    "    classifier_oracle_value_0 = thresholds_data[fair_constarint[fair]][0]['Non-White']\n",
    "else: \n",
    "    classifier_oracle_value_1 = thresholds_data[fair_constarint[fair]][0]\n",
    "    classifier_oracle_value_0 = thresholds_data[fair_constarint[fair]][0]\n",
    "#print (classifier_oracle_value_1, classifier_oracle_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f249293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the LB_1 and UB_1 with fair classifier(Using alpha = 60 for label 0 and median for label 1)\n",
    "temp = 2*beta.cdf(beta.ppf(reference_quantile_0,a0_group1,b0_group1,loc=min_0_group1, scale=max_0_group1-min_0_group1),a0_group1,b0_group1,loc=min_0_group1, scale=max_0_group1-min_0_group1) -\\\n",
    "            beta.cdf(classifier_group1,a0_group1,b0_group1,loc=min_0_group1, scale=max_0_group1-min_0_group1)\n",
    "LB_group1 = max(min_0_group1, float(beta.ppf(temp,a0_group1,b0_group1,loc=min_0_group1, scale=max_0_group1-min_0_group1)))\n",
    "\n",
    "temp = 2*beta.cdf(beta.ppf(reference_quantile_1,a1_group1,b1_group1,loc=min_1_group1, scale=max_1_group1-min_1_group1),a1_group1,b1_group1,loc=min_1_group1, scale=max_1_group1-min_1_group1) -\\\n",
    "            beta.cdf(LB_group1,a1_group1,b1_group1,loc=min_1_group1, scale=max_1_group1-min_1_group1)\n",
    "UB_group1 = min(max_1_group1, float(beta.ppf(temp,a1_group1,b1_group1,loc=min_1_group1, scale=max_1_group1-min_1_group1)))\n",
    "\n",
    "# Find the LB_0 and UB_0 with fair classifier(Using alpha = 60 for label 0 and median for label 1)\n",
    "temp = 2*beta.cdf(beta.ppf(reference_quantile_0,a0_group0,b0_group0,loc=min_0_group0, scale=max_0_group0-min_0_group0),a0_group0,b0_group0,loc=min_0_group0, scale=max_0_group0-min_0_group0) -\\\n",
    "            beta.cdf(classifier_group0,a0_group0,b0_group0,loc=min_0_group0, scale=max_0_group0-min_0_group0)\n",
    "LB_group0 = max(min_0_group0, float(beta.ppf(temp,a0_group0,b0_group0,loc=min_0_group0, scale=max_0_group0-min_0_group0)))\n",
    "\n",
    "temp = 2*beta.cdf(beta.ppf(reference_quantile_1,a1_group0,b1_group0,loc=min_1_group0, scale=max_1_group0-min_1_group0),a1_group0,b1_group0,loc=min_1_group0, scale=max_1_group0-min_1_group0) -\\\n",
    "            beta.cdf(LB_group0,a1_group0,b1_group0,loc=min_1_group0, scale=max_1_group0-min_1_group0)\n",
    "UB_group0 = min(max_1_group0, float(beta.ppf(temp,a1_group0,b1_group0,loc=min_1_group0, scale=max_1_group0-min_1_group0)))\n",
    "print (LB_group1, UB_group1, LB_group0, UB_group0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f21fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "data_1_group1 = Initialhigh_label1\n",
    "data_0_group1 = Initialhigh_label0\n",
    "data_1_group0 = Initiallow_label1\n",
    "data_0_group0 = Initiallow_label0\n",
    "\n",
    "Rest_data = original.tail(round((1-Initial_fit_portion)*len(features)))[[\"Prob\",\"Target\",\"Race\"]]\n",
    "Rest_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "info = pd.DataFrame(columns = ['True Label','Values','Decision','RP_1_group1','RP_0_group1',\n",
    "                                      'Classifier_group1','Explore_Prob_group1','RP_1_group0','RP_0_group0',\n",
    "                                      'Classifier_group0','Explore_Prob_group0','Alpha_1_group1','Alpha_0_group1',\n",
    "                                       'Alpha_1_group0','Alpha_0_group0','Regret','Race'])\n",
    "info[[\"Values\",\"True Label\",\"Race\"]] = Rest_data\n",
    "info = shuffle(info)\n",
    "info.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70812e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd58353",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "# Create loop for updating\n",
    "while (i <= len(info)-1):\n",
    "    \n",
    "    if i<= (len(grouphigh_label1) + len(grouphigh_label0) + len(grouplow_label1) + len(grouplow_label1)):\n",
    "        batchsize = batchsize_init\n",
    "    else: \n",
    "        batchsize = batchsize_additional\n",
    "        \n",
    "    data_1_trun_group1 = []\n",
    "    data_0_trun_group1 = []\n",
    "    data_1_trun_group0 = []\n",
    "    data_0_trun_group0 = []\n",
    "    \n",
    "    # find quantile of reference point including label 0 tail for advantage group\n",
    "    portion_right0_group1 = (beta.sf(beta.ppf(reference_quantile_0,a0_group1,b0_group1,loc=min_0_group1, scale=max_0_group1-min_0_group1),\n",
    "                                    a0_group1,b0_group1,loc=min_0_group1, scale=max_0_group1-min_0_group1)\n",
    "                     /beta.sf(LB_group1,a0_group1,b0_group1,loc=min_0_group1, scale=max_0_group1-min_0_group1))\n",
    "    portion_left0_group1 = 1 - portion_right0_group1\n",
    "    \n",
    "    portion_right1_group1 = (beta.sf(beta.ppf(reference_quantile_1,a1_group1,b1_group1,loc=min_1_group1, scale=max_1_group1-min_1_group1),\n",
    "                                    a1_group1,b1_group1,loc=min_1_group1, scale=max_1_group1-min_1_group1)\n",
    "                     /beta.sf(LB_group1,a1_group1,b1_group1,loc=min_1_group1, scale=max_1_group1-min_1_group1))\n",
    "    portion_left1_group1 = 1 - portion_right1_group1\n",
    "    \n",
    "    # find quantile of reference point including label 0 tail for disadvantage group\n",
    "    portion_right0_group0 = (beta.sf(beta.ppf(reference_quantile_0,a0_group0,b0_group0,loc=min_0_group0, scale=max_0_group0-min_0_group0),\n",
    "                                    a0_group0,b0_group0,loc=min_0_group0, scale=max_0_group0-min_0_group0)\n",
    "                     /beta.sf(LB_group0,a0_group0,b0_group0,loc=min_0_group0, scale=max_0_group0-min_0_group0))\n",
    "    portion_left0_group0 = 1 - portion_right0_group0\n",
    "    \n",
    "    portion_right1_group0 = (beta.sf(beta.ppf(reference_quantile_1,a1_group0,b1_group0,loc=min_1_group0, scale=max_1_group0-min_1_group0),\n",
    "                                    a1_group0,b1_group0,loc=min_1_group0, scale=max_1_group0-min_1_group0)\n",
    "                     /beta.sf(LB_group0,a1_group0,b1_group0,loc=min_1_group0, scale=max_1_group0-min_1_group0))\n",
    "    portion_left1_group0 = 1 - portion_right1_group0\n",
    "    \n",
    "    k1_group1 = len(data_1_trun_group1)\n",
    "    k0_group1 = len(data_0_trun_group1)\n",
    "    k1_group0 = len(data_1_trun_group0)\n",
    "    k0_group0 = len(data_0_trun_group0)\n",
    "    #loop into each batch\n",
    "    while ((min(k1_group1, k0_group1, k1_group0, k0_group0) <= batchsize) & (i <= len(info)-1)):\n",
    "        \n",
    "        if info.loc[i,'Race'] == 1:\n",
    "        \n",
    "            # Make decisions\n",
    "            if (info.loc[i,'Values'] >= classifier_group1) & (info.loc[i,'Values']<=UB_group1):\n",
    "                info.loc[i,'Decision'] = 1\n",
    "            elif (info.loc[i,'Values'] < classifier_group1) & (info.loc[i,'Values']>=LB_group1) & (np.random.uniform(size=1)<=exploration_porb_group1):\n",
    "                info.loc[i,'Decision'] = 1\n",
    "            elif (info.loc[i,'Values'] >= classifier_group1) & (info.loc[i,'Values']>UB_group1):\n",
    "                info.loc[i,'Decision'] = 2\n",
    "            elif (info.loc[i,'Values'] < classifier_group1) & (info.loc[i,'Values']<LB_group1):\n",
    "                info.loc[i,'Decision'] = 3\n",
    "            else:\n",
    "                info.loc[i,'Decision'] = 0  \n",
    "\n",
    "            # Record confusion matrix\n",
    "            if (info.loc[i,'True Label']==1) & ((info.loc[i,'Decision']==1) or (info.loc[i,'Decision']==2)):\n",
    "                TP_group1 += 1\n",
    "            elif (info.loc[i,'True Label']==1) & ((info.loc[i,'Decision']==0) or (info.loc[i,'Decision']==3)):\n",
    "                FN_group1 += 1\n",
    "            elif (info.loc[i,'True Label']==0) & ((info.loc[i,'Decision']==0) or (info.loc[i,'Decision']==3)):\n",
    "                TN_group1 += 1\n",
    "            else:\n",
    "                FP_group1 += 1\n",
    "\n",
    "            # Record oracle matrix\n",
    "            if (info.loc[i,'Values'] >= classifier_oracle_value_1) & (info.loc[i,'True Label']==1):\n",
    "                TP_oracle_group1 += 1\n",
    "            elif (info.loc[i,'Values'] >= classifier_oracle_value_1) & (info.loc[i,'True Label']==0):\n",
    "                FP_oracle_group1 += 1\n",
    "            elif (info.loc[i,'Values'] < classifier_oracle_value_1) & (info.loc[i,'True Label']==1):\n",
    "                FN_oracle_group1 += 1\n",
    "            else:\n",
    "                TN_oracle_group1 += 1\n",
    "\n",
    "\n",
    "            # Record label 1 information for updating\n",
    "            if (info.loc[i,'True Label']==1) & ((info.loc[i,'Decision']==1) or (info.loc[i,'Decision']==2)):\n",
    "                data_1_group1 = np.append(data_1_group1, info.loc[i,'Values'])\n",
    "                if (((info.loc[i,'Values'] < classifier_group1) & (info.loc[i,'Decision']==1)) \n",
    "                   or ((info.loc[i,'Values'] >= classifier_group1) & (info.loc[i,'Decision']==1) & (np.random.uniform(size=1)<=exploration_porb_group1))):\n",
    "                    data_1_trun_group1 = np.append(data_1_trun_group1, info.loc[i,'Values'])\n",
    "            else:\n",
    "                data_1_group1 = data_1_group1 \n",
    "                data_1_trun_group1 = data_1_trun_group1\n",
    "\n",
    "\n",
    "            # Record label 0 information for updating\n",
    "            if (info.loc[i,'True Label']==0) & ((info.loc[i,'Decision']==1) or (info.loc[i,'Decision']==2)):\n",
    "                data_0_group1 = np.append(data_0_group1, info.loc[i,'Values'])\n",
    "                if (((info.loc[i,'Values'] < classifier_group1) & (info.loc[i,'Decision']==1)) \n",
    "                    or ((info.loc[i,'Values'] >= classifier_group1) & (np.random.uniform(size=1)<=exploration_porb_group1))):\n",
    "                    data_0_trun_group1 = np.append(data_0_trun_group1, info.loc[i,'Values'])\n",
    "            else:\n",
    "                data_0_group1 = data_0_group1 \n",
    "                data_0_trun_group1 = data_0_trun_group1\n",
    "       \n",
    "        else: # for non-white group\n",
    "            \n",
    "            # Make decisions\n",
    "            if (info.loc[i,'Values'] >= classifier_group0) & (info.loc[i,'Values']<=UB_group0):\n",
    "                info.loc[i,'Decision'] = 1\n",
    "            elif (info.loc[i,'Values'] < classifier_group0) & (info.loc[i,'Values']>=LB_group0) & (np.random.uniform(size=1)<=exploration_porb_group0):\n",
    "                info.loc[i,'Decision'] = 1\n",
    "            elif (info.loc[i,'Values'] >= classifier_group0) & (info.loc[i,'Values']>UB_group0):\n",
    "                info.loc[i,'Decision'] = 2\n",
    "            elif (info.loc[i,'Values'] < classifier_group0) & (info.loc[i,'Values']<LB_group0):\n",
    "                info.loc[i,'Decision'] = 3\n",
    "            else:\n",
    "                info.loc[i,'Decision'] = 0  \n",
    "\n",
    "            # Record confusion matrix\n",
    "            if (info.loc[i,'True Label']==1) & ((info.loc[i,'Decision']==1) or (info.loc[i,'Decision']==2)):\n",
    "                TP_group0 += 1\n",
    "            elif (info.loc[i,'True Label']==1) & ((info.loc[i,'Decision']==0) or (info.loc[i,'Decision']==3)):\n",
    "                FN_group0 += 1\n",
    "            elif (info.loc[i,'True Label']==0) & ((info.loc[i,'Decision']==0) or (info.loc[i,'Decision']==3)):\n",
    "                TN_group0 += 1\n",
    "            else:\n",
    "                FP_group0 += 1\n",
    "\n",
    "            # Record oracle matrix\n",
    "            if (info.loc[i,'Values'] >= classifier_oracle_value_0) & (info.loc[i,'True Label']==1):\n",
    "                TP_oracle_group0 += 1\n",
    "            elif (info.loc[i,'Values'] >= classifier_oracle_value_0) & (info.loc[i,'True Label']==0):\n",
    "                FP_oracle_group0 += 1\n",
    "            elif (info.loc[i,'Values'] < classifier_oracle_value_0) & (info.loc[i,'True Label']==1):\n",
    "                FN_oracle_group0 += 1\n",
    "            else:\n",
    "                TN_oracle_group0 += 1\n",
    "\n",
    "\n",
    "            # Record label 1 information for updating\n",
    "            if (info.loc[i,'True Label']==1) & ((info.loc[i,'Decision']==1) or (info.loc[i,'Decision']==2)):\n",
    "                data_1_group0 = np.append(data_1_group0, info.loc[i,'Values'])\n",
    "                if (((info.loc[i,'Values'] < classifier_group0) & (info.loc[i,'Decision']==1)) \n",
    "                   or ((info.loc[i,'Values'] >= classifier_group0) & (info.loc[i,'Decision']==1) & (np.random.uniform(size=1)<=exploration_porb_group0))):\n",
    "                    data_1_trun_group0 = np.append(data_1_trun_group0, info.loc[i,'Values'])\n",
    "            else:\n",
    "                data_1_group0 = data_1_group0 \n",
    "                data_1_trun_group0 = data_1_trun_group0\n",
    "\n",
    "\n",
    "            # Record label 0 information for updating\n",
    "            if (info.loc[i,'True Label']==0) & ((info.loc[i,'Decision']==1) or (info.loc[i,'Decision']==2)):\n",
    "                data_0_group0 = np.append(data_0_group0, info.loc[i,'Values'])\n",
    "                if (((info.loc[i,'Values'] < classifier_group0) & (info.loc[i,'Decision']==1)) \n",
    "                    or ((info.loc[i,'Values'] >= classifier_group0) & (np.random.uniform(size=1)<=exploration_porb_group0))):\n",
    "                    data_0_trun_group0 = np.append(data_0_trun_group0, info.loc[i,'Values'])\n",
    "            else:\n",
    "                data_0_group0 = data_0_group0 \n",
    "                data_0_trun_group0 = data_0_trun_group0\n",
    "            \n",
    "        k1_group1 = len(data_1_trun_group1)\n",
    "        k0_group1 = len(data_0_trun_group1)\n",
    "        k1_group0 = len(data_1_trun_group0)\n",
    "        k0_group0 = len(data_0_trun_group0)\n",
    "        i = i + 1\n",
    "    \n",
    "    print (k1_group1, k0_group1, k1_group0, k0_group0)\n",
    "    # Record new mean value/RP and classifier\n",
    "    info.loc[i-1,'RP_1_group1'] = np.quantile(data_1_trun_group1, portion_left1_group1)  #np.median(data_1_trun_group1) \n",
    "    info.loc[i-1,'RP_0_group1'] =  np.quantile(data_0_trun_group1, portion_left0_group1) \n",
    "    info.loc[i-1,'Classifier_group1'] = classifier_group1\n",
    "    info.loc[i-1,'Regret'] = (FP_group1 + FN_group1)-(FP_oracle_group1 + FN_oracle_group1)\n",
    "    info.loc[i-1,'Explore_Prob_group1'] = exploration_porb_group1\n",
    "    \n",
    "    info.loc[i-1,'RP_1_group0'] = np.quantile(data_1_trun_group0, portion_left1_group0)   #np.median(data_1_trun_group0)  \n",
    "    info.loc[i-1,'RP_0_group0'] =  np.quantile(data_0_trun_group0, portion_left0_group0) \n",
    "    info.loc[i-1,'Classifier_group0'] = classifier_group0\n",
    "    info.loc[i-1,'Explore_Prob_group0'] = exploration_porb_group0\n",
    "    \n",
    "    x = np.arange(0.01,10,0.01)\n",
    "    # Reassign new values to parameters\n",
    "    def f(alpha):\n",
    "        return abs(beta.cdf(info.loc[i-1,'RP_1_group1'], alpha, b1_group1, loc=min_1_group1, scale=max_1_group1-min_1_group1) - reference_quantile_1)\n",
    "    info.loc[i-1,'Alpha_1_group1'] = np.argmin(f(x))*0.01\n",
    "    \n",
    "    def f(alpha):\n",
    "        return abs(beta.cdf(info.loc[i-1,'RP_0_group1'], alpha, b0_group1, loc=min_0_group1, scale=max_0_group1-min_0_group1) - reference_quantile_0)\n",
    "    info.loc[i-1,'Alpha_0_group1'] = np.argmin(f(x))*0.01\n",
    "    \n",
    "    def f(alpha):\n",
    "        return abs(beta.cdf(info.loc[i-1,'RP_1_group0'], alpha, b1_group0, loc=min_1_group0, scale=max_1_group0-min_1_group0) - reference_quantile_1)\n",
    "    info.loc[i-1,'Alpha_1_group0'] = np.argmin(f(x))*0.01\n",
    "    \n",
    "    def f(alpha):\n",
    "        return abs(beta.cdf(info.loc[i-1,'RP_0_group0'], alpha, b0_group0, loc=min_0_group0, scale=max_0_group0-min_0_group0) - reference_quantile_0)\n",
    "    info.loc[i-1,'Alpha_0_group0'] = np.argmin(f(x))*0.01\n",
    "    \n",
    "    a1_group1 = info.loc[i-1,'Alpha_1_group1']\n",
    "    a0_group1 = info.loc[i-1,'Alpha_0_group1']\n",
    "    a1_group0 = info.loc[i-1,'Alpha_1_group0']\n",
    "    a0_group0 = info.loc[i-1,'Alpha_0_group0']\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Update fair classifiers, UB and LB\n",
    "    # Find the parameters proportions, base_rate and base_rates \n",
    "    proportions = {'White': (len(data_1_group1) + len(data_0_group1))/(len(data_1_group1) + len(data_0_group1) + len(data_1_group0) + len(data_0_group0)), \n",
    "                   'Non-White': 1 - (len(data_1_group1) + len(data_0_group1))/(len(data_1_group1) + len(data_0_group1) + len(data_1_group0) + len(data_0_group0))}\n",
    "    base_rate = (len(data_1_group1) + len(data_1_group0))/(len(data_1_group1) + len(data_0_group1) + len(data_1_group0) + len(data_0_group0))\n",
    "    d = {'White': len(data_1_group1)/(len(data_1_group1) + len(data_0_group1)), 'Non-White': len(data_1_group0)/(len(data_1_group0) + len(data_0_group0))}\n",
    "    base_rates = pd.Series(data = d, index = ['White','Non-White'])\n",
    "\n",
    "    # Find the ROC curve by construting confusion matrix \n",
    "    yprob1 = np.append(data_1_group1, data_0_group1)\n",
    "    yprob0 = np.append(data_1_group0, data_0_group0)\n",
    "\n",
    "    TPR1 = []\n",
    "    FPR1 = []\n",
    "    threshold1 = []\n",
    "    for threshold in range(201,0,-1):\n",
    "        cm, TPR, FPR = CM(np.append(np.ones(len(data_1_group1)),np.zeros(len(data_0_group1))),yprob1,threshold/200)\n",
    "        TPR1 = TPR1 + [TPR]\n",
    "        FPR1 = FPR1 + [FPR]\n",
    "        threshold1 = threshold1 + [threshold/200]\n",
    "    TPR1 = np.array(TPR1)\n",
    "    FPR1 = np.array(FPR1)\n",
    "    threshold1 = np.array(threshold1)\n",
    "\n",
    "    TPR0 = []\n",
    "    FPR0 = []\n",
    "    threshold0 = []\n",
    "    for threshold in range(201,0,-1):\n",
    "        cm, TPR, FPR = CM(np.append(np.ones(len(data_1_group0)),np.zeros(len(data_0_group0))),yprob0,threshold/200)\n",
    "        TPR0 = TPR0 + [TPR]\n",
    "        FPR0 = FPR0 + [FPR]\n",
    "        threshold0 = threshold0 + [threshold/200]\n",
    "    TPR0 = np.array(TPR0)\n",
    "    FPR0 = np.array(FPR0)\n",
    "    threshold0 = np.array(threshold0)\n",
    "\n",
    "    # Find the parameter rocs \n",
    "    rocs = {'White': (FPR1, TPR1, threshold1), 'Non-White': (FPR0, TPR0,threshold0)}\n",
    "\n",
    "    # Find the initial fairness classifier_1 and classifier_0 \n",
    "    thresholds_data = find_thresholds(rocs, proportions, base_rate, base_rates, COST_MATRIX)\n",
    "    if fair != 0:\n",
    "        classifier_group1 = thresholds_data[fair_constarint[fair]][0]['White']\n",
    "        classifier_group0 = thresholds_data[fair_constarint[fair]][0]['Non-White']\n",
    "    else: \n",
    "        classifier_group1 = thresholds_data[fair_constarint[fair]][0]\n",
    "        classifier_group0 = thresholds_data[fair_constarint[fair]][0]\n",
    "\n",
    "    temp = 2*beta.cdf(beta.ppf(reference_quantile_0,a0_group1,b0_group1,loc=min_0_group1, scale=max_0_group1-min_0_group1),a0_group1,b0_group1,loc=min_0_group1, scale=max_0_group1-min_0_group1) -\\\n",
    "            beta.cdf(classifier_group1,a0_group1,b0_group1,loc=min_0_group1, scale=max_0_group1-min_0_group1)\n",
    "    LB_group1 = max(min_0_group1, float(beta.ppf(temp,a0_group1,b0_group1,loc=min_0_group1, scale=max_0_group1-min_0_group1)))\n",
    "\n",
    "    temp = 2*beta.cdf(beta.ppf(reference_quantile_1,a1_group1,b1_group1,loc=min_1_group1, scale=max_1_group1-min_1_group1),a1_group1,b1_group1,loc=min_1_group1, scale=max_1_group1-min_1_group1) -\\\n",
    "            beta.cdf(LB_group1,a1_group1,b1_group1,loc=min_1_group1, scale=max_1_group1-min_1_group1)\n",
    "    UB_group1 = min(max_1_group1, float(beta.ppf(temp,a1_group1,b1_group1,loc=min_1_group1, scale=max_1_group1-min_1_group1)))\n",
    "    \n",
    "\n",
    "    temp = 2*beta.cdf(beta.ppf(reference_quantile_0,a0_group0,b0_group0,loc=min_0_group0, scale=max_0_group0-min_0_group0),a0_group0,b0_group0,loc=min_0_group0, scale=max_0_group0-min_0_group0) -\\\n",
    "            beta.cdf(classifier_group0,a0_group0,b0_group0,loc=min_0_group0, scale=max_0_group0-min_0_group0)\n",
    "    LB_group0 = max(min_0_group0, float(beta.ppf(temp,a0_group0,b0_group0,loc=min_0_group0, scale=max_0_group0-min_0_group0)))\n",
    "\n",
    "    temp = 2*beta.cdf(beta.ppf(reference_quantile_1,a1_group0,b1_group0,loc=min_1_group0, scale=max_1_group0-min_1_group0),a1_group0,b1_group0,loc=min_1_group0, scale=max_1_group0-min_1_group0) -\\\n",
    "            beta.cdf(LB_group0,a1_group0,b1_group0,loc=min_1_group0, scale=max_1_group0-min_1_group0)\n",
    "    UB_group0 = min(max_1_group0, float(beta.ppf(temp,a1_group0,b1_group0,loc=min_1_group0, scale=max_1_group0-min_1_group0)))\n",
    "    \n",
    "    # Update exploration probability for advantage group\n",
    "    theoretical_value_group1 = beta.sf(classifier_group1,a0_group1,b0_group1,loc=min_0_group1, scale=max_0_group1-min_0_group1) \\\n",
    "                                /beta.sf(LB_group1,a0_group1,b0_group1,loc=min_0_group1, scale=max_0_group1-min_0_group1)\n",
    "    experiment_value_group1 = len(data_0_trun_group1[data_0_trun_group1>=classifier_group1])/len(data_0_trun_group1[data_0_trun_group1>=LB_group1])\n",
    "    diff_group1 = abs(theoretical_value_group1 - experiment_value_group1)\n",
    "     \n",
    "    exploration_porb_group1 = 1 - 0.1*(i//(len(info)/10))\n",
    "    # Update exploration probability for disadvantage group\n",
    "    theoretical_value_group0 = beta.sf(classifier_group0,a0_group0,b0_group0,loc=min_0_group0, scale=max_0_group0-min_0_group0) \\\n",
    "                                /beta.sf(LB_group0,a0_group0,b0_group0,loc=min_0_group0, scale=max_0_group0-min_0_group0)\n",
    "    experiment_value_group0 = len(data_0_trun_group0[data_0_trun_group0>=classifier_group0])/len(data_0_trun_group0[data_0_trun_group0>=LB_group0])\n",
    "    diff_group0 = abs(theoretical_value_group0 - experiment_value_group0)\n",
    "    exploration_porb_group0 = 1 - 0.1*(i//(len(info)/10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
